\documentclass[]{article}
\usepackage[a4paper, total={6.5in, 8.5in}]{geometry}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Deep V-Learning for Pool-Based Active Learning}

\begin{document}

\maketitle

\section{Introduction}
Even though pool-based Active Learning (P-AL) is the more popular setting for Active Learning, it creates significant challenges when reinforcement learning is applied to it.
The most prominent one is that both Q-Learning and Policy Gradient Methods require a fixed action space. 
In the P-AL setting the action space has the same size as the pool of unlabeled images to choose from (or a subsample thereof).
Even for simple datasets P-AL methods require a samplesize of $>$100 to work effectively, creating a large action space for the reinforcement learning agent. \\

\subsection{Contribution}
\begin{itemize}
	\item First V-Learning Approach for P-AL
	\item Variable sample sizes to fit every dataset
	\item (Dataset and model agnostic state space) \textit{Probably not the first}
\end{itemize}

\section{Related Work}

\section{Background}
\subsection{V-Learning}
V-Learning (\cite{rl_intro} p.119) is closely related to Q-Learning. It estimates the value of states rather that the value of all actions given a state (Q-Learning \cite{rl_intro} p.131) 
This requires the environment to provide possible future states $s_{t+1} \in S_{t+1}$ given any state $s_t$.
A policy $\pi_v$ typically chooses the most promising future state $argmax \hspace{1mm} V(s_{t+1})$ \\
Temporal difference (TD) learning for a neural network parametrized by $\theta$ gives us a nearly identical update formula compared to Q-Learning
\begin{equation}
	\theta \leftarrow \theta + \eta \left( r_t + \gamma \hat V_\theta(s_{t+1}) \right)
\end{equation} 

\subsection{Active Learning as a V-Learning MDP}
We change the usual MDP formulation from predecessing literature, which, based on a presented sample of unlabeled samples of size $k$, defines a state space $S := \mathbb{R}^{k \times f}$ with features $f$ and consequently an action space of $A := \{ 1, \ldots, k \}$. \\ [1mm]
We instead define the state space to be 1-dimensional and use the batch dimension $b$ for a variable sample size.
\begin{equation}
	S := \mathbb{R}^{3+|Z|} := [\text{BvsSB, Entropy, F1, Z}]
\end{equation}
V-Learning does not defines an action space, since the actions are implicit, as defined by the policy $\pi_v$. \\
This formulation poses a problem when storing transitions. The starting state $s_t$ inuitively will be choosen datapoint $s_{t, a_t}$, but since we don't want to store the full follow-up state $s_{t+1} \in \mathbb{R}^{b \times 3+|Z|}$ we use the average over the batch-dimension.
This serves as a proxy of the impact of the implicit action $a_t$ on the follow-up state $s_{t+1}$.\\
This results in a stored transition $\phi_t := \{ s_{t, a_t}, r_t, \bar{s}_{t+1}, d_t \}$ where $\bar{s}_{t+1}$ is the average of the follow-up state and $d_t$ indicates wether a terminal state was reached.

\section{Methodology}
\subsection{Model and Reinforcement Learning}
We use a MLP with a single output neuron $f_\theta : \mathbb{R}^{3+|Z|} \rightarrow \mathbb{R}$ as agent network.
As stated before, the sample of presented datapoints uses the batch dimension for sample size, resulting in $b$ point-wise predictions of the agent. \\
For fitting the agent we use a target network $f_{\theta^-}$ \cite{double_q_learning}, an n-step return (\cite{rl_intro} p. 142) with $n=3$ and priorizited experience replay \cite{prioritized_replay}.


\section{Dueling Networks for dynamic Action Spaces}


\bibliographystyle{plain}
\bibliography{main.bib} 


\end{document}